{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22618,
     "status": "ok",
     "timestamp": 1747216938703,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "zINVxZI3WoHV",
    "outputId": "a277ee6f-ce0f-4537-a343-8a5e8351e971"
   },
   "outputs": [],
   "source": [
    "#@title Install PYIR and setup the database\n",
    "\n",
    "# Install Biopython & PyIR\n",
    "%pip install crowelab_pyir\n",
    "%pip install Biopython\n",
    "%pip install XlsxWriter\n",
    "!pip install mpl-chord-diagram\n",
    "# Builds databases in pyir library directory\n",
    "!pyir setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1073,
     "status": "ok",
     "timestamp": 1747218061989,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "K6TWN3IISw8o",
    "outputId": "cb933f42-08ea-40b7-8b65-e5318aae3a74"
   },
   "outputs": [],
   "source": [
    "#@title Import Libraries and define input & output\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from crowelab_pyir import PyIR\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_chord_diagram import chord_diagram\n",
    "import seaborn as sns\n",
    "import regex as re  # Use regex module for fuzzy matching\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Define a list to store plot images globally\n",
    "plot_images = []\n",
    "\n",
    "def save_plot_to_memory(fig, dpi=500):\n",
    "    \"\"\"Save matplotlib figure to memory for Excel export.\"\"\"\n",
    "    img = io.BytesIO()\n",
    "    fig.savefig(img, format='png', bbox_inches='tight', dpi=dpi)\n",
    "    img.seek(0)\n",
    "    plot_images.append(img)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "folder_path = \"/content/drive/Shared drives/BIIE_PROGRAM-LABS/DEEPIR/3_RESEARCH/IRP-110_MAMMALIAN-AB/1_PILOT-PHASE/2_DATA-FILES/Sanger_Seq_Clones/4395396_4395387\" #@param {type:\"string\"}\n",
    "output_csv_file = \"CTLA4_high_binders\"  #@param {type:\"string\")\n",
    "sanger_quality_threshold = 30  #@param {type:\"string\"}\n",
    "remove_superimposed_signal_reads = True   #@param {type:\"string\"}\n",
    "sanger_report_superimposed_signal_reads = \"()\"  #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ2Q9qPrurYi"
   },
   "source": [
    "## Phred Quality Score\n",
    "Phred quality score, or q-score, is a quality measure that estimates the probability that a base was called incorrectly, given on a negative log scale (Q=âˆ’log10P(incorrect)) so that a higher q-score indicates a more confident base call.\n",
    "\n",
    "\n",
    "* Phred score of 50 indicates that there is 99.999% that this base has been assigned *incorrectly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747217036838,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "lJT4BYjkunlu"
   },
   "outputs": [],
   "source": [
    "def get_phred_quality(folder_path, plot=False, sanger_quality_threshold=20):\n",
    "    \"\"\"\n",
    "    Reads all .ab1 files in the specified folder and extracts sequencing quality data.\n",
    "    If plot=True, generates and saves a histogram of average Phred quality scores.\n",
    "    \"\"\"\n",
    "\n",
    "    quality_data = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.ab1'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            record = SeqIO.read(file_path, \"abi\")\n",
    "\n",
    "            quality_scores = record.letter_annotations[\"phred_quality\"]\n",
    "            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0\n",
    "            clone_name = filename.rsplit(\".ab1\", 1)[0]\n",
    "            threshold_status = \"pass\" if avg_quality > sanger_quality_threshold else \"failed\"\n",
    "\n",
    "            quality_data.append({\n",
    "                \"File Name\": filename,\n",
    "                \"Clone Name\": clone_name,\n",
    "                \"Avg Quality\": avg_quality,\n",
    "                \"Threshold\": threshold_status\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(quality_data)\n",
    "\n",
    "    if plot and not df.empty:\n",
    "        fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        sns.histplot(df[\"Avg Quality\"], bins=100, kde=False, ax=ax1)\n",
    "        ax1.set_xlabel(\"Average Sequencing Quality\")\n",
    "        ax1.set_ylabel(\"Frequency\")\n",
    "        ax1.set_title(\"Distribution of Average Phred Sequencing Quality\")\n",
    "        ax1.axvline(sanger_quality_threshold, color='blue', linestyle='dotted', linewidth=2,\n",
    "                   label=f'Threshold: {sanger_quality_threshold}')\n",
    "        ax1.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        save_plot_to_memory(fig1, dpi=600)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 10961,
     "status": "ok",
     "timestamp": 1747217048122,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "PwGiBGf9unq7",
    "outputId": "24ece474-3cb4-477a-d006-fbbb5894af18"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_quality = get_phred_quality(folder_path, plot=True, sanger_quality_threshold=sanger_quality_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747217048900,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "Ad6LKucpW18q",
    "outputId": "25fcefa2-ee54-42ab-b9ac-54dcb5d5ee1c"
   },
   "outputs": [],
   "source": [
    "# Parse superimposed reads\n",
    "if isinstance(sanger_report_superimposed_signal_reads, str):\n",
    "    try:\n",
    "        superimposed_clones = list(eval(sanger_report_superimposed_signal_reads))\n",
    "    except:\n",
    "        superimposed_clones = []\n",
    "else:\n",
    "    superimposed_clones = list(sanger_report_superimposed_signal_reads)\n",
    "\n",
    "#List of failed clones\n",
    "failed_clones = df_quality[df_quality['Threshold'] == 'failed']['Clone Name'].tolist()\n",
    "\n",
    "#Remove reads that were reported with \"superimposed signals\" by Nanopore and very short in length\n",
    "exclusion_prefixes = failed_clones.copy()\n",
    "\n",
    "if remove_superimposed_signal_reads:\n",
    "    exclusion_prefixes += superimposed_clones\n",
    "\n",
    "if exclusion_prefixes:\n",
    "    print(\"The following clones will be filtered from the quality processing of the reads:\")\n",
    "    for clone in exclusion_prefixes:\n",
    "        print(f\" - {clone}\")\n",
    "else:\n",
    "    print(\"No clones to filter out based on quality threshold.\")\n",
    "\n",
    "# Count excluded entries\n",
    "excluded_from_df = df_quality[df_quality[\"Clone Name\"].str.contains('|'.join(exclusion_prefixes))] if exclusion_prefixes else pd.DataFrame()\n",
    "num_excluded_clones = excluded_from_df[\"Clone Name\"].nunique()\n",
    "print(f\"Total number of sequence IDs excluded due to quality: {num_excluded_clones}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr6cJH5du3pY"
   },
   "source": [
    "# Processing of the .fasta files from Sanger Sequencing\n",
    "##Fetch Ab Framework Sequences using PyIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13177,
     "status": "ok",
     "timestamp": 1747217066189,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "RIEW0NjHunwo",
    "outputId": "8c05fbe2-9414-489d-b9b3-bcb03ca2400f"
   },
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "import os\n",
    "\n",
    "output_fasta = os.path.join(folder_path, \"combined_parts.fasta\")\n",
    "\n",
    "\n",
    "with open(output_fasta, 'w') as out_handle:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if ((\"PR186\" in filename) or (\"PR171\" in filename)) and filename.endswith(\".fasta\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "                # Skip records whose ID starts with any excluded prefix that didnt pass the QC\n",
    "                if record.id.startswith(tuple(exclusion_prefixes)):\n",
    "                  continue\n",
    "                seq_str = str(record.seq)\n",
    "                if \"PR171\" in filename:\n",
    "                    front_seq = seq_str[:700]\n",
    "                    description_suffix = \"[front 700bp]\"\n",
    "                elif \"PR186\" in filename:\n",
    "                    front_seq = seq_str[:500]\n",
    "                    description_suffix = \"[front 500bp]\"\n",
    "                else:\n",
    "                    front_seq = seq_str\n",
    "                    description_suffix = \"[full sequence]\"\n",
    "\n",
    "                front_record = SeqRecord(\n",
    "                    Seq(front_seq),\n",
    "                    id=record.id,\n",
    "                    description=record.description + \" \" + description_suffix\n",
    "                )\n",
    "                SeqIO.write(front_record, out_handle, \"fasta\")\n",
    "def predict_frameworks(input_file):\n",
    "    pyir = PyIR(\n",
    "        query=input_file,\n",
    "        args=[\n",
    "            '--input_type', 'fasta',\n",
    "            '--outfmt', 'dict',\n",
    "            '--receptor', 'Ig',\n",
    "            '--species', 'human'\n",
    "        ]\n",
    "    )\n",
    "    result = pyir.run()\n",
    "    df = pd.DataFrame.from_dict(result, orient='index')\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "#-------------------------------------------------------------------------------------------------->\n",
    "df = predict_frameworks(output_fasta)\n",
    "#Keep necessary columns\n",
    "df_reduced = df[['sequence_id', 'sequence', 'locus', 'sequence_alignment', 'sequence_alignment_aa', 'v_family', 'd_family', 'j_family',\n",
    "                  'cdr1_aa', 'cdr2_aa', 'cdr3_aa', 'cdr3_aa_length', 'j_sequence_alignment', 'v_identity', 'd_identity', 'j_identity']] #j_sequece_alignment will be needed to reconstruct the j gene\n",
    "cols = ['v_identity', 'd_identity', 'j_identity']\n",
    "\n",
    "df_reduced[cols] = df_reduced[cols].apply(pd.to_numeric, errors='coerce')\n",
    "#<--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Im4EVnKSDRk_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b06Mq3wZUBW"
   },
   "source": [
    "#Rretrieve heavy & light chain sequence with nuceotide mismatch allowance in the search *motifs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1747217069078,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "0WBJh41vzKWM",
    "outputId": "41999932-fad2-45f1-a6de-a614b4de2a2f"
   },
   "outputs": [],
   "source": [
    "def sanger_reads_to_dict(input_folder):\n",
    "    seq_dict = {}\n",
    "    seq_lengths = {}\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if (\"PR186\" in filename or \"PR171\" in filename) and filename.endswith(\".fasta\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "                seq_dict[record.id] = str(record.seq)\n",
    "                seq_lengths[record.id] = len(record.seq)\n",
    "    return seq_dict, seq_lengths\n",
    "\n",
    "def trim_sequence_by_id(seq_id, seq):\n",
    "    \"\"\"\n",
    "    Trims sequences based on start and end motifs and returns only the sequence between them (excluding motifs).\n",
    "\n",
    "    - PR171: Extracts the sequence between motifs **as-is**, ensuring length >600.\n",
    "    - PR186: Extracts the sequence and **returns its reverse complement**, ensuring length >300.\n",
    "    - If no exact match is found for a motif, allows **1 mismatch**, but only if the length condition is met.\n",
    "    - If the first mismatch attempt does not fulfill the length condition, tries another mismatch combination.\n",
    "    - If still unsuccessful, searches for the second occurrence of the start or end motif with 1 mismatch at a different position.\n",
    "    \"\"\"\n",
    "    if not isinstance(seq, str) or pd.isna(seq):  # Handle missing sequences\n",
    "        return \"\", 0\n",
    "\n",
    "    if \"PR186\" in seq_id:\n",
    "        start_motif = \"GTGACC\"\n",
    "        end_motif = \"GGGGCCGGG\"\n",
    "        min_length = 300\n",
    "        reverse_complement = True\n",
    "    elif \"PR171\" in seq_id:\n",
    "        start_motif = \"GCCACCATG\"\n",
    "        end_motif = \"CTGGAGAC\"\n",
    "        min_length = 600\n",
    "        reverse_complement = False\n",
    "    else:\n",
    "        return \"\", 0  # If not recognized, return empty\n",
    "\n",
    "    def find_motif_with_mismatch(motif, sequence, occurrence=1):\n",
    "        exact_matches = [m.end() for m in re.finditer(motif, sequence)]\n",
    "        if len(exact_matches) >= occurrence:\n",
    "            return exact_matches[occurrence - 1]\n",
    "\n",
    "        mismatch_matches = [m.end() for m in re.finditer(f'({motif}){{s<=1}}', sequence)]\n",
    "        if len(mismatch_matches) >= occurrence:\n",
    "            return mismatch_matches[occurrence - 1]\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Try to find start motif\n",
    "    start_idx = find_motif_with_mismatch(start_motif, seq)\n",
    "    if start_idx is None:\n",
    "        return \"\", 0\n",
    "\n",
    "    remaining_seq = seq[start_idx:]\n",
    "\n",
    "    # Try to find end motif\n",
    "    end_idx = None\n",
    "    for mismatch_count in range(2):  # Try 0 or 1 mismatch\n",
    "        end_matches = [m.start() for m in re.finditer(f'({end_motif}){{s<={mismatch_count}}}', remaining_seq)]\n",
    "        if end_matches:\n",
    "            for end_position in end_matches:\n",
    "                trimmed_seq = seq[start_idx:start_idx + end_position]\n",
    "                if len(trimmed_seq) > min_length:\n",
    "                    end_idx = start_idx + end_position\n",
    "                    break  # Stop if valid length is found\n",
    "        if end_idx:\n",
    "            break\n",
    "\n",
    "    # If length is still not fulfilled, search for the second occurrence of start or end motif\n",
    "    if end_idx is None:\n",
    "        start_idx_alt = find_motif_with_mismatch(start_motif, seq, occurrence=2)\n",
    "        if start_idx_alt:\n",
    "            remaining_seq_alt = seq[start_idx_alt:]\n",
    "            for mismatch_count in range(2):\n",
    "                end_matches_alt = [m.start() for m in re.finditer(f'({end_motif}){{s<={mismatch_count}}}', remaining_seq_alt)]\n",
    "                if end_matches_alt:\n",
    "                    for end_position_alt in end_matches_alt:\n",
    "                        trimmed_seq_alt = seq[start_idx_alt:start_idx_alt + end_position_alt]\n",
    "                        if len(trimmed_seq_alt) > min_length:\n",
    "                            start_idx, end_idx = start_idx_alt, start_idx_alt + end_position_alt\n",
    "                            break\n",
    "                if end_idx:\n",
    "                    break\n",
    "\n",
    "    if end_idx is None:\n",
    "        return \"\", 0\n",
    "\n",
    "    trimmed_seq = seq[start_idx:end_idx]\n",
    "\n",
    "    # Apply reverse complement if needed (only for PR186)\n",
    "    if reverse_complement:\n",
    "        trimmed_seq = str(Seq(trimmed_seq).reverse_complement())\n",
    "\n",
    "    return trimmed_seq, len(trimmed_seq)\n",
    "\n",
    "\n",
    "def process_sanger_reads(df, input_folder, sanger_reads_to_dict, trim_sequence_by_id):\n",
    "    \"\"\"\n",
    "    Processes Sanger sequencing reads by mapping original sequences and lengths,\n",
    "    and applying sequence trimming.\n",
    "    \"\"\"\n",
    "    # Build the dictionary of full original sequences and their lengths\n",
    "    sanger_read_dict, sanger_length_dict = sanger_reads_to_dict(input_folder)\n",
    "\n",
    "    # Store original sequences and lengths in DataFrame\n",
    "    df['original_sanger_read'] = df['sequence_id'].map(sanger_read_dict)\n",
    "    df['original_length'] = df['sequence_id'].map(sanger_length_dict)\n",
    "\n",
    "    # Apply trimming and store processed sequences and lengths\n",
    "    df['processed_sanger_read'], df['processed_length'] = zip(*df.apply(\n",
    "        lambda row: trim_sequence_by_id(row['sequence_id'], row['original_sanger_read']), axis=1\n",
    "    ))\n",
    "    return df\n",
    "#-------------------------------------------------------------------------------------------------->\n",
    "df_reduced = process_sanger_reads(df_reduced, folder_path, sanger_reads_to_dict, trim_sequence_by_id)\n",
    "#<--------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Function to find motifs with up to 2 mismatches but only in either start or end motif (not both)\n",
    "def find_motifs_with_mismatches(sequence, start_motif, end_motif, allow_mismatch_in_start=True):\n",
    "    \"\"\"\n",
    "    Searches for start and end motifs allowing mismatches in only one of them.\n",
    "    \"\"\"\n",
    "    if allow_mismatch_in_start:\n",
    "        start_regex = f\"({start_motif}){{s<=2}}\"  # Allow mismatches in start motif\n",
    "        end_regex = f\"({end_motif})\"  # Exact match for end motif\n",
    "    else:\n",
    "        start_regex = f\"({start_motif})\"  # Exact match for start motif\n",
    "        end_regex = f\"({end_motif}){{s<=2}}\"  # Allow mismatches in end motif\n",
    "\n",
    "    start_match = re.search(start_regex, sequence)\n",
    "    end_match = re.search(end_regex, sequence)\n",
    "\n",
    "    if start_match and end_match:\n",
    "        start_pos = start_match.start()\n",
    "        end_pos = end_match.end()\n",
    "        return sequence[start_pos:end_pos], end_pos - start_pos\n",
    "    return \"\", 0\n",
    "\n",
    "def find_second_motif(sequence, motif, is_start=True):\n",
    "    \"\"\"\n",
    "    Finds the second occurrence of the given motif in the sequence and extracts the subsequence.\n",
    "\n",
    "    \"\"\"\n",
    "    matches = list(re.finditer(motif, sequence))\n",
    "\n",
    "    if len(matches) < 2:\n",
    "        return \"\", 0  # Not enough motif occurrences\n",
    "\n",
    "    if is_start:\n",
    "        # Take the second start motif position and slice from there to the end\n",
    "        start_pos = matches[1].start()\n",
    "        return sequence[start_pos:], len(sequence[start_pos:])\n",
    "    else:\n",
    "        # Take the second end motif position and slice from start to there\n",
    "        end_pos = matches[1].end()\n",
    "        return sequence[:end_pos], len(sequence[:end_pos])\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------->\n",
    "\n",
    "# Recheck for sequences where processed_length is 0 or below the thresholds\n",
    "\n",
    "df_recheck = df_reduced[\n",
    "    ((df_reduced[\"processed_length\"] == 0) |\n",
    "     ((df_reduced[\"processed_length\"] < 600) & df_reduced[\"sequence_id\"].str.contains(\"PR171\")) |\n",
    "     ((df_reduced[\"processed_length\"] < 300) & df_reduced[\"sequence_id\"].str.contains(\"PR186\")))\n",
    "].copy()\n",
    "# Total number of sequences flagged for rechecking\n",
    "total_recheck = len(df_recheck)\n",
    "\n",
    "# Count how many had exactly 0 length\n",
    "zero_length_count = (df_reduced[\"processed_length\"] == 0).sum()\n",
    "\n",
    "# Count how many PR171 were below threshold but not zero\n",
    "below_threshold_pr171 = ((df_reduced[\"processed_length\"] < 600) &\n",
    "                         (df_reduced[\"processed_length\"] > 0) &\n",
    "                         df_reduced[\"sequence_id\"].str.contains(\"PR171\")).sum()\n",
    "\n",
    "# Count how many PR186 were below threshold but not zero\n",
    "below_threshold_pr186 = ((df_reduced[\"processed_length\"] < 300) &\n",
    "                         (df_reduced[\"processed_length\"] > 0) &\n",
    "                         df_reduced[\"sequence_id\"].str.contains(\"PR186\")).sum()\n",
    "\n",
    "# Print the summary\n",
    "print(\"\\nðŸ” Summary of Sequences Needing Reprocessing:\")\n",
    "print(f\"  - Total flagged for recheck: {total_recheck}\")\n",
    "print(f\"  - Sequences with processed length = 0: {zero_length_count}\")\n",
    "print(f\"  - PR171 sequences below 600 bp: {below_threshold_pr171}\")\n",
    "print(f\"  - PR186 sequences below 300 bp: {below_threshold_pr186}\")\n",
    "\n",
    "# Initialize counters\n",
    "total_pr186 = 0\n",
    "total_pr171 = 0\n",
    "pr186_mismatch_count = 0\n",
    "pr171_mismatch_count = 0\n",
    "\n",
    "# Process only rechecked rows\n",
    "for idx, row in df_recheck.iterrows():\n",
    "    seq_id = row[\"sequence_id\"]\n",
    "    sequence = row[\"original_sanger_read\"]\n",
    "\n",
    "    if \"PR186\" in seq_id:\n",
    "        total_pr186 += 1\n",
    "        start_motif = \"GTGACC\"\n",
    "        end_motif = \"GGGGCCGGG\"\n",
    "    elif \"PR171\" in seq_id:\n",
    "        total_pr171 += 1\n",
    "        start_motif = \"GCCACCATG\"\n",
    "        end_motif = \"CTGGAGAC\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    matched = False\n",
    "\n",
    "    # Step 1: Try allowing mismatches only in the start motif\n",
    "    processed_seq, processed_len = find_motifs_with_mismatches(sequence, start_motif, end_motif, allow_mismatch_in_start=True)\n",
    "    if processed_len >= 600 and \"PR171\" in seq_id:\n",
    "        matched = True\n",
    "    elif processed_len >= 300 and \"PR186\" in seq_id:\n",
    "        matched = True\n",
    "\n",
    "    # Step 2: Try allowing mismatches only in the end motif\n",
    "    if not matched:\n",
    "        processed_seq, processed_len = find_motifs_with_mismatches(sequence, start_motif, end_motif, allow_mismatch_in_start=False)\n",
    "        if processed_len >= 600 and \"PR171\" in seq_id:\n",
    "            matched = True\n",
    "        elif processed_len >= 300 and \"PR186\" in seq_id:\n",
    "            matched = True\n",
    "\n",
    "    # Step 3: Try second occurrence of start motif\n",
    "    if not matched:\n",
    "        processed_seq, processed_len = find_second_motif(sequence, start_motif, is_start=True)\n",
    "        if processed_len >= 600 and \"PR171\" in seq_id:\n",
    "            matched = True\n",
    "        elif processed_len >= 300 and \"PR186\" in seq_id:\n",
    "            matched = True\n",
    "\n",
    "    # Step 4: Try second occurrence of end motif\n",
    "    if not matched:\n",
    "        processed_seq, processed_len = find_second_motif(sequence, end_motif, is_start=False)\n",
    "        if processed_len >= 600 and \"PR171\" in seq_id:\n",
    "            matched = True\n",
    "        elif processed_len >= 300 and \"PR186\" in seq_id:\n",
    "            matched = True\n",
    "\n",
    "    # Track mismatched usage\n",
    "    if not matched:\n",
    "        if \"PR186\" in seq_id:\n",
    "            pr186_mismatch_count += 1\n",
    "        elif \"PR171\" in seq_id:\n",
    "            pr171_mismatch_count += 1\n",
    "\n",
    "    # Update DataFrame\n",
    "    df_recheck.at[idx, \"processed_sanger_read\"] = processed_seq\n",
    "    df_recheck.at[idx, \"processed_length\"] = processed_len\n",
    "\n",
    "# Total PR186 and PR171 sequences in the full dataset\n",
    "total_all_pr186 = df_reduced[\"sequence_id\"].str.contains(\"PR186\").sum()\n",
    "total_all_pr171 = df_reduced[\"sequence_id\"].str.contains(\"PR171\").sum()\n",
    "\n",
    "# Final fallback counts from earlier\n",
    "# pr186_mismatch_count, pr171_mismatch_count already calculated from recheck loop\n",
    "\n",
    "# Calculate percentages relative to full dataset\n",
    "percent_pr186_all = (pr186_mismatch_count / total_all_pr186) * 100 if total_all_pr186 > 0 else 0\n",
    "percent_pr171_all = (pr171_mismatch_count / total_all_pr171) * 100 if total_all_pr171 > 0 else 0\n",
    "\n",
    "print(\"\\nðŸ“Š Summary of Fallback Usage (as % of all sequences):\")\n",
    "print(f\"  - PR186: {pr186_mismatch_count}/{total_all_pr186} sequences = {percent_pr186_all:.2f}% needing fallback\")\n",
    "print(f\"  - PR171: {pr171_mismatch_count}/{total_all_pr171} sequences = {percent_pr171_all:.2f}% needing fallback\")\n",
    "\n",
    "# Merge the updated values back into the original DataFrame\n",
    "df_reduced.update(df_recheck)\n",
    "#df_reduced.to_csv('/content/drive/Shared drives/BIIE_PROGRAM-LABS/DEEPIR/3_RESEARCH/IRP-110_MAMMALIAN-AB/1_PILOT-PHASE/2_DATA-FILES/Sanger_Seq_Clones/4318714_4314914/df_reduced_new_.csv')\n",
    "\n",
    "#<--------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Stich last part of J gene\n",
    "def stitch_j_gene(df, motif=\"GGTCAC\"):\n",
    "    \"\"\"\n",
    "    Matches the motif 'GGTCAC' between the processed_sanger_read and the j_sequence_alignment\n",
    "    for _PR186 sequences and appends the remaining part of the j gene to processed_sanger_read.\n",
    "    \"\"\"\n",
    "    def append_j_gene(row):\n",
    "        if \"PR186\" in row[\"sequence_id\"] and isinstance(row[\"j_sequence_alignment\"], str):\n",
    "            # Find the motif in the sequence\n",
    "            match = re.search(motif, row[\"j_sequence_alignment\"])\n",
    "            if match:\n",
    "                start_idx = match.start()\n",
    "                remaining_j_gene = row[\"j_sequence_alignment\"][start_idx:]\n",
    "\n",
    "                 # Extract sequence from motif onward\n",
    "                return row[\"processed_sanger_read\"] + remaining_j_gene\n",
    "\n",
    "        return row[\"processed_sanger_read\"]  # Return unmodified for non-PR186\n",
    "\n",
    "    # Apply function to update processed_sanger_read\n",
    "    df[\"processed_sanger_read\"] = df.apply(append_j_gene, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#                if updated_sequence.endswith(\"G\"):\n",
    "        #            updated_sequence = updated_sequence[:-1]\n",
    "              #  return updated_sequence\n",
    "       # return row[\"processed_sanger_read\"]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------->\n",
    "df_reduced = stitch_j_gene(df_reduced)\n",
    "#<--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuGCCbTSZqNa"
   },
   "source": [
    "#Merge heavy&light chain reads and check for gene identity coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1747217076951,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "Pz5i3dmWPVbp"
   },
   "outputs": [],
   "source": [
    "remove_identity_failures = True  #@param {type:\"boolean\"}\n",
    "remove_length_failures = True  #@param {type:\"boolean\"}\n",
    "remove_stopcodon_failures = False  #@param {type:\"boolean\"}\n",
    "remove_repeated_genes = True #@param {type:\"boolean\"}\n",
    "remove_clonotype_duplicates=True  #@param {type:\"boolean\"}\n",
    "prefix = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1747217310735,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "0A4AkcZH1ynA",
    "outputId": "283ea57b-399c-4738-cf8b-35490e4f61b3"
   },
   "outputs": [],
   "source": [
    "def merge_sanger_reads(df,\n",
    "                       remove_identity_failures=remove_identity_failures,\n",
    "                       remove_length_failures=remove_length_failures,\n",
    "                       remove_stopcodon_failures=remove_stopcodon_failures,\n",
    "                       remove_repeated_genes=remove_repeated_genes,\n",
    "                       remove_clonotype_duplicates=remove_clonotype_duplicates):\n",
    "    \"\"\"\n",
    "    Merges Sanger sequencing reads by combining heavy (PR186) and light (PR171) chains into a single row.\n",
    "    Adds QC flags and clonotype frequency, with options to filter out:\n",
    "      - Length <80 aa\n",
    "      - Identity <80%\n",
    "      - Stop codons\n",
    "      - Repeated gene segments\n",
    "      - Clonotype duplicates (Frequency > 1)\n",
    "    Prints summary statistics and final counts.\n",
    "    Returns the filtered and annotated DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # ------------------- Merge Paired Reads -------------------\n",
    "\n",
    "\n",
    "\n",
    "    df[\"clone_name\"] = df[\"sequence_id\"].str.replace(r'(_\\d+)?_PR\\d+$', '', regex=True)\n",
    "    df_heavy = df[df['sequence_id'].str.contains('_PR186')].copy()\n",
    "    df_light = df[df['sequence_id'].str.contains('_PR171')].copy()\n",
    "\n",
    "    df_light = df_light.rename(columns=lambda x: x + \"_light\" if x not in [\"clone_name\", \"sequence_id\"] else x)\n",
    "    df_heavy = df_heavy.rename(columns=lambda x: x + \"_heavy\" if x not in [\"clone_name\", \"sequence_id\"] else x)\n",
    "\n",
    "    df_merged = pd.merge(df_heavy, df_light, on=\"clone_name\", how=\"outer\")\n",
    "\n",
    "    # ------------------- Build Recombination Info -------------------\n",
    "    recomb_cols = ['v_family_heavy', 'd_family_heavy', 'j_family_heavy', 'v_family_light', 'j_family_light']\n",
    "    df_merged[\"info\"] = df_merged[recomb_cols].apply(lambda x: \"/\".join(str(i) for i in x), axis=1)\n",
    "\n",
    "    # ------------------- Add QC Flags -------------------\n",
    "    df_merged['length<80'] = (\n",
    "        (df_merged['sequence_alignment_aa_heavy'].str.len() < 80) |\n",
    "        (df_merged['sequence_alignment_aa_light'].str.len() < 80)\n",
    "    )\n",
    "\n",
    "    identity_cols = ['v_identity', 'd_identity', 'j_identity']\n",
    "    for col in identity_cols:\n",
    "        for chain in ['heavy', 'light']:\n",
    "            colname = f\"{col}_{chain}\"\n",
    "            if colname in df_merged.columns:\n",
    "                df_merged[colname] = pd.to_numeric(df_merged[colname], errors='coerce')\n",
    "\n",
    "    df_merged['identity<80%'] = (\n",
    "        (df_merged['v_identity_heavy'] < 80) |\n",
    "        (df_merged['d_identity_heavy'] < 80) |\n",
    "        (df_merged['j_identity_heavy'] < 80) |\n",
    "        (df_merged['v_identity_light'] < 80) |\n",
    "        (df_merged['d_identity_light'] < 80) |\n",
    "        (df_merged['j_identity_light'] < 80)\n",
    "    )\n",
    "\n",
    "    df_merged['sequence_alignment_aa_heavy'] = df_merged['sequence_alignment_aa_heavy'].astype(str)\n",
    "    df_merged['sequence_alignment_aa_light'] = df_merged['sequence_alignment_aa_light'].astype(str)\n",
    "    df_merged['stopcodon_present'] = (\n",
    "        df_merged['sequence_alignment_aa_heavy'].str.contains(r'\\*', na=False) |\n",
    "        df_merged['sequence_alignment_aa_light'].str.contains(r'\\*', na=False)\n",
    "    )\n",
    "\n",
    "    df_merged[\"repeated_gene_present\"] = df_merged[\"info\"].apply(\n",
    "        lambda x: len(x.split('/')) != len(set(x.split('/')))\n",
    "    )\n",
    "\n",
    "    # ------------------- Clonotype Frequency -------------------\n",
    "    group_cols = ['v_family_heavy', 'd_family_heavy', 'j_family_heavy', 'cdr1_aa_heavy',\n",
    "                  'cdr2_aa_heavy', 'cdr3_aa_heavy', 'cdr1_aa_light', 'cdr2_aa_light',\n",
    "                  'v_family_light', 'j_family_light', 'cdr3_aa_light']\n",
    "\n",
    "    frequency_df = df_merged.groupby(group_cols).size().reset_index(name='Frequency')\n",
    "    df_merged = df_merged.merge(frequency_df, on=group_cols, how='left')\n",
    "\n",
    "    # ------------------- Print QC and Frequency Stats -------------------\n",
    "    print(f\"Number of clones that did not pass the QC (Phred):\", len(exclusion_prefixes))\n",
    "    print(f\"Number of clones with aa sequence length < 80: {df_merged['length<80'].sum()}\")\n",
    "    print(f\"Number of clones with v/d/j identity < 80%: {df_merged['identity<80%'].sum()}\")\n",
    "    print(f\"Number of clones with stop codons present: {df_merged['stopcodon_present'].sum()}\")\n",
    "    print(f\"Number of clones with repeated gene segments: {df_merged['repeated_gene_present'].sum()}\")\n",
    "\n",
    "    total_clones = len(df_merged)\n",
    "    num_unique_clonotypes = frequency_df.shape[0]\n",
    "    num_duplicates = total_clones - num_unique_clonotypes\n",
    "    percent_unique = (num_unique_clonotypes / total_clones) * 100\n",
    "    percent_duplicates = 100 - percent_unique\n",
    "\n",
    "    print(f\"Î¤otal number of clonotypes: {total_clones}\")\n",
    "    print(f\"Number of duplicate clonotypes: {num_duplicates}\")\n",
    "    print(f\"Percentage of duplicates: {percent_duplicates:.2f}%\")\n",
    "    print(f\"\\nTotal number of unique clonotypes: {num_unique_clonotypes}\")\n",
    "    print(f\"Percentage of unique remaining clones: {percent_unique:.2f}%\")\n",
    "\n",
    "\n",
    "    # Print duplicate clone names\n",
    "    high_freq = df_merged[df_merged[\"Frequency\"] > 1][[\"clone_name\", \"info\", \"Frequency\"]].drop_duplicates()\n",
    "    if not high_freq.empty:\n",
    "        print(\"\\nClone names with repeated clonotypes:\")\n",
    "        print(high_freq.to_string(index=False))\n",
    "\n",
    "    # ------------------- Apply Filtering -------------------\n",
    "    filter_mask = pd.Series(False, index=df_merged.index)\n",
    "    # Apply filtering based on all selected QC flags\n",
    "    filter_mask = pd.Series(False, index=df_merged.index)\n",
    "\n",
    "    if remove_length_failures:\n",
    "        filter_mask |= df_merged['length<80']\n",
    "    if remove_identity_failures:\n",
    "        filter_mask |= df_merged['identity<80%']\n",
    "    if remove_stopcodon_failures:\n",
    "        filter_mask |= df_merged['stopcodon_present']\n",
    "    if remove_repeated_genes:\n",
    "        filter_mask |= df_merged['repeated_gene_present']\n",
    "    #if remove_clonotype_duplicates:\n",
    "     #   filter_mask |= df_merged.duplicated(subset=group_cols, keep='first')\n",
    "    if remove_clonotype_duplicates:\n",
    "        filter_mask |= df_merged['Frequency'] > 1\n",
    "\n",
    "    num_removed = filter_mask.sum()\n",
    "    num_remaining = (~filter_mask).sum()\n",
    "\n",
    "\n",
    "    # ------------------- Report which duplicate clones are kept ------------------\n",
    "    duplicates_df = df_merged[df_merged[\"Frequency\"] > 1].copy()\n",
    "    duplicate_resolution_records = []\n",
    "    duplicate_resolution_df = pd.DataFrame()  # Ensure this is always defined\n",
    "\n",
    "    if remove_clonotype_duplicates:\n",
    "        for _, group in duplicates_df.groupby(group_cols):\n",
    "            clone_ids = group[\"clone_name\"].unique().tolist()\n",
    "            kept_clone = clone_ids[0]\n",
    "            duplicate_clones = clone_ids[1:]\n",
    "            recomb = group[\"info\"].iloc[0]\n",
    "            if duplicate_clones:\n",
    "              duplicate_resolution_records.append({\n",
    "              \"Kept Clone\": kept_clone,\n",
    "             \"Removed Clones\": \", \".join(duplicate_clones),\n",
    "              \"Clonotype\": recomb\n",
    "          })\n",
    "            print(f\" - Keeping {kept_clone}, removing {', '.join(duplicate_clones)} (clonotype: {recomb})\")\n",
    "\n",
    "            print(f\" - Keeping {kept_clone}, removing {', '.join(duplicate_clones)} (clonotype: {recomb})\")\n",
    "            print(f\"\\nTotal clones removed after filtering: {num_removed}\")\n",
    "            print(f\"Total clones remaining: {num_remaining}\")\n",
    "            duplicate_resolution_df = pd.DataFrame(duplicate_resolution_records)\n",
    "    else:\n",
    "      print(f\"\\nClonotype deduplication is disabled.\")\n",
    "      print(f\"Total clones removed after filtering: {num_removed}\")\n",
    "      print(f\"Total clones remaining: {num_remaining}\")\n",
    "\n",
    "    # ------------------- Remove duplicate clonotypes from output -------------------\n",
    "    # Keep only the first clone per unique clonotype\n",
    "    df_merged = df_merged.drop_duplicates(subset=group_cols, keep='first')\n",
    "\n",
    "    df_merged.rename(columns={\n",
    "        \"sequence_alignment_heavy\": \"vh_dna\",\n",
    "        \"sequence_alignment_light\": \"vl_dna\",\n",
    "        \"sequence_alignment_aa_heavy\": \"vh_aa\",\n",
    "        \"sequence_alignment_aa_light\": \"vl_aa\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Define columns to keep if not keeping all columns\n",
    "    columns_to_keep = [\n",
    "        \"clone_name\", \"vh_dna\",  \"vh_aa\", \"locus_heavy\",\n",
    "        \"vl_dna\", \"vl_aa\", \"locus_light\",\n",
    "        \"processed_sanger_read_heavy\", \"processed_sanger_read_light\",\"info\", \"identity<80%\", \"length<80\", \"stopcodon_present\", \"repeated_gene_present\"\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # Keep specific columns\n",
    "\n",
    "    df_processed = df_merged[columns_to_keep]\n",
    "\n",
    "\n",
    "    # Substitute \"Clone\" with \"C\" in clone_name\n",
    "    df_processed[\"clone_name\"] = df_processed[\"clone_name\"].astype(str).str.replace(r'^clone', 'C', regex=True)\n",
    "\n",
    "    # Add prefix to clone_name\n",
    "    df_processed[\"clone_name\"] = df_processed[\"clone_name\"].apply(lambda x: f\"{prefix}{x}\" if isinstance(x, str) else x)\n",
    "\n",
    "    # Natural sorting function to order C1, C2, C3 instead of C1, C10, C2\n",
    "    def natural_sort_key(text):\n",
    "        return [int(num) if num.isdigit() else num for num in re.split(r'(\\d+)', text)]\n",
    "\n",
    "    # Sort DataFrame by clone_name using natural sorting\n",
    "    df_processed = df_processed.sort_values(by=\"clone_name\", key=lambda x: x.map(natural_sort_key))\n",
    "\n",
    "    # ------------------- Collect summary statistics in a DataFrame ------------------\n",
    "\n",
    "    # ---- Create summary section as DataFrame ----\n",
    "    summary_stats = [\n",
    "    [\"Number of clones that did not pass the QC\", len(exclusion_prefixes)],\n",
    "    [\"Number of clones with aa sequence length < 80\", df_merged['length<80'].sum()],\n",
    "    [\"Number of clones with v/d/j identity < 80%\", df_merged['identity<80%'].sum()],\n",
    "    [\"Number of clones with stop codons present\", df_merged['stopcodon_present'].sum()],\n",
    "    [\"Number of clones with repeated gene segments\", df_merged['repeated_gene_present'].sum()],\n",
    "    [\"\", \"\"],\n",
    "    [\"Number of duplicate clonotypes\", num_duplicates],\n",
    "    [\"Percentage of duplicates\", f\"{percent_duplicates:.2f}%\"],\n",
    "    [\"Total number of unique clonotypes\", num_unique_clonotypes],\n",
    "    [\"Percentage of unique remaining clones\", f\"{percent_unique:.2f}%\"],\n",
    "    [\"\", \"\"],\n",
    "    [\"Total clones removed after filtering\", num_removed],\n",
    "    [\"Total unique clones remaining\", num_remaining],\n",
    "\n",
    "]\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_stats, columns=[\"Metric\", \"Value\"])\n",
    "\n",
    "    # ---- Clone names with repeated clonotypes ----\n",
    "    high_freq = df_merged[df_merged[\"Frequency\"] > 1][[\"clone_name\", \"info\", \"Frequency\"]].drop_duplicates()\n",
    "    repeated_clones_df = high_freq.rename(columns={\"clone_name\": \"Clone with Duplicates\"})\n",
    "\n",
    "    return df_processed[~filter_mask].copy(), df_merged[~filter_mask].copy(), summary_df, repeated_clones_df, duplicate_resolution_df\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------->\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    df_processed, df_merged, summary_df, repeated_clones_df, duplicate_resolution_df = merge_sanger_reads(df_reduced)\n",
    "#<--------------------------------------------------------------------------------------------------\n",
    "\n",
    "def process_merged_data(df_merged, keep_all_columns=False, save_csv=False, output_path=\"/content/drive/MyDrive/processed_data.csv\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Processes the merged Sanger sequencing DataFrame by formatting and filtering columns, and optionally saving the data as a CSV file.\n",
    "    Allows adding a prefix to the clone name and sorting clone names numerically.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------- Sequence Processing ------------------\n",
    "\n",
    "    #Define adaptor sequence (PaqCl + attP + Kozak + ATG), added extra \"AA\" to distinguish easier the start\n",
    "    adaptor_seq_light = \"AACACCTGCGCCCATCAGTGGTTTGTCTGGTCAACCACCGCGGACTCAGTGGTGTACGGTACAAACCCAGCCGCCACCATG\"\n",
    "    # Define P2A sequence\n",
    "    P2A = \"CTGCTGAAGCAGGCTGGAGACGTGGAAGAGAACCCCGGCCCC\"\n",
    "    #Define adaptor sequence (w PaqCl)\n",
    "    adaptor_seq_heavy = \"GGTAAGCAGGTGTT\"\n",
    "\n",
    "    #Remove \"G\" at the end of the heavy chain sequence since it doesnt belong to a codon\n",
    "    df_processed = df_merged.copy()\n",
    "    df_processed[\"processed_sanger_read_heavy\"] = df_processed[\"processed_sanger_read_heavy\"].apply(lambda x: x[:-1] if isinstance(x, str) and x.endswith(\"G\") else x)\n",
    "    df_processed[\"vh_dna\"] = df_processed [\"vh_dna\"].apply(lambda x: x[:-1] if isinstance(x, str) and x.endswith(\"G\") else x)\n",
    "\n",
    "    # Construct full Sanger read with P2A sequence in between\n",
    "    df_processed[\"full_sanger_read\"] = adaptor_seq_light +  df_processed[\"processed_sanger_read_light\"] + P2A + df_processed[\"processed_sanger_read_heavy\"] + adaptor_seq_heavy\n",
    "\n",
    "    #Format dataframe\n",
    "    #df_final = df_processed.copy()\n",
    "    df_processed = df_processed.drop(columns=[ 'processed_sanger_read_heavy', 'processed_sanger_read_light'])\n",
    "    df_processed = df_processed.rename(columns={'full_sanger_read': 'insert_seq'})\n",
    "    df_processed = df_processed[[\"clone_name\", \"insert_seq\", \"vh_dna\", \"vl_dna\", \"vh_aa\", \"vl_aa\", \"info\"]]\n",
    "    # Save DataFrame to CSV if requested\n",
    "    if save_csv:\n",
    "        df_processed.to_csv(output_path, index=False)\n",
    "\n",
    "    return df_processed\n",
    "#-------------------------------------------------------------------------------------------------->\n",
    "df_merged_b = process_merged_data(df_processed)\n",
    "#<--------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fREt3lWqDuDf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omIiW24jvg5N"
   },
   "source": [
    "# Visualize *Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "executionInfo": {
     "elapsed": 18162,
     "status": "ok",
     "timestamp": 1747217345950,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "73IdW2LOvhuT",
    "outputId": "33a273cd-87ff-48aa-9061-fc69f5bddc63"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------->\n",
    "# Define required columns\n",
    "required_columns = ['v_family_heavy', 'd_family_heavy', 'j_family_heavy']\n",
    "path_counts = df_merged.groupby(required_columns).size().reset_index(name='count')\n",
    "\n",
    "# Extract unique gene families\n",
    "v_genes = path_counts[\"v_family_heavy\"].dropna().unique().tolist()\n",
    "d_genes = path_counts[\"d_family_heavy\"].dropna().unique().tolist()\n",
    "j_genes = path_counts[\"j_family_heavy\"].dropna().unique().tolist()\n",
    "all_genes = v_genes + d_genes + j_genes\n",
    "gene_idx = {gene: i for i, gene in enumerate(all_genes)}\n",
    "\n",
    "# Initialize adjacency matrices\n",
    "matrix_v_d = np.zeros((len(all_genes), len(all_genes)))\n",
    "matrix_v_j = np.zeros((len(all_genes), len(all_genes)))\n",
    "matrix_j_d = np.zeros((len(all_genes), len(all_genes)))\n",
    "\n",
    "# Populate matrices\n",
    "for _, row in path_counts.iterrows():\n",
    "    v, d, j, count = row[\"v_family_heavy\"], row[\"d_family_heavy\"], row[\"j_family_heavy\"], row[\"count\"]\n",
    "    if v in gene_idx and d in gene_idx:\n",
    "        matrix_v_d[gene_idx[v], gene_idx[d]] += count\n",
    "    if v in gene_idx and j in gene_idx:\n",
    "        matrix_v_j[gene_idx[v], gene_idx[j]] += count\n",
    "    if j in gene_idx and d in gene_idx:\n",
    "        matrix_j_d[gene_idx[j], gene_idx[d]] += count\n",
    "\n",
    "# Ensure symmetry\n",
    "matrix_v_d += matrix_v_d.T\n",
    "matrix_v_j += matrix_v_j.T\n",
    "matrix_j_d += matrix_j_d.T\n",
    "\n",
    "def filter_empty_genes(matrix, names):\n",
    "    nonzero = np.any(matrix > 0, axis=0)\n",
    "    return matrix[np.ix_(nonzero, nonzero)], [name for i, name in enumerate(names) if nonzero[i]]\n",
    "\n",
    "matrix_v_d, names_v_d = filter_empty_genes(matrix_v_d, all_genes)\n",
    "matrix_v_j, names_v_j = filter_empty_genes(matrix_v_j, all_genes)\n",
    "matrix_j_d, names_j_d = filter_empty_genes(matrix_j_d, all_genes)\n",
    "\n",
    "# Create plots\n",
    "fig2, axes2 = plt.subplots(1, 3, figsize=(24, 8), dpi=600)\n",
    "chord_diagram(matrix_v_d, names=names_v_d, sort=\"size\", rotate_names=True, fontsize=8, cmap=\"twilight\", ax=axes2[0])\n",
    "axes2[0].set_title(\"V-D Genes\", fontsize=12, y=1.1)\n",
    "\n",
    "chord_diagram(matrix_v_j, names=names_v_j, sort=\"size\", rotate_names=True, fontsize=8, cmap=\"twilight\", ax=axes2[1])\n",
    "axes2[1].set_title(\"V-J Genes\", fontsize=12, y=1.1)\n",
    "\n",
    "chord_diagram(matrix_j_d, names=names_j_d, sort=\"size\", rotate_names=True, fontsize=8, cmap=\"twilight\", ax=axes2[2])\n",
    "axes2[2].set_title(\"J-D Genes\", fontsize=12, y=1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#fig2.savefig(\"chord_diagrams_light.png\", dpi=500, bbox_inches='tight', format='png')\n",
    "\n",
    "save_plot_to_memory(fig2, dpi=600)\n",
    "\n",
    "#<--------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "executionInfo": {
     "elapsed": 6001,
     "status": "ok",
     "timestamp": 1747217385268,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "X8fUALT-hJ9g",
    "outputId": "7f25f554-23e9-4ef9-c1c2-a7708dcda373"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------->\n",
    "\n",
    "def prepare_vj_matrix(df_light):\n",
    "    light_path_counts = df_light.groupby(['v_family_light', 'j_family_light']).size().reset_index(name='count')\n",
    "\n",
    "    v_genes = light_path_counts[\"v_family_light\"].dropna().unique().tolist()\n",
    "    j_genes = light_path_counts[\"j_family_light\"].dropna().unique().tolist()\n",
    "    all_genes = v_genes + j_genes\n",
    "\n",
    "    gene_idx = {gene: i for i, gene in enumerate(all_genes)}\n",
    "    matrix = np.zeros((len(all_genes), len(all_genes)))\n",
    "\n",
    "    for _, row in light_path_counts.iterrows():\n",
    "        v, j, count = row[\"v_family_light\"], row[\"j_family_light\"], row[\"count\"]\n",
    "        if v in gene_idx and j in gene_idx:\n",
    "            matrix[gene_idx[v], gene_idx[j]] += count\n",
    "\n",
    "    matrix = matrix + matrix.T  # Make symmetric\n",
    "\n",
    "    # Filter out unused genes\n",
    "    nonzero = np.any(matrix > 0, axis=0)\n",
    "    filtered_matrix = matrix[np.ix_(nonzero, nonzero)]\n",
    "    filtered_genes = [gene for i, gene in enumerate(all_genes) if nonzero[i]]\n",
    "\n",
    "    return filtered_matrix, filtered_genes\n",
    "\n",
    "# -----------------------\n",
    "# Prepare data for IGK and IGL\n",
    "# -----------------------\n",
    "df_igk = df_merged[df_merged[\"locus_light\"] == \"IGK\"]\n",
    "df_igl = df_merged[df_merged[\"locus_light\"] == \"IGL\"]\n",
    "\n",
    "matrix_igk, genes_igk = prepare_vj_matrix(df_igk)\n",
    "matrix_igl, genes_igl = prepare_vj_matrix(df_igl)\n",
    "\n",
    "# Count number of IGK and IGL clones\n",
    "num_igk = df_igk[\"clone_name\"].nunique()\n",
    "num_igl = df_igl[\"clone_name\"].nunique()\n",
    "\n",
    "print(f\"Number of IGK (kappa) light chain clones: {num_igk}\")\n",
    "print(f\"Number of IGL (lambda) light chain clones: {num_igl}\")\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Plot side-by-side chord diagrams\n",
    "# -----------------------\n",
    "\n",
    "fig3, axes3 = plt.subplots(1, 2, figsize=(24, 8))\n",
    "\n",
    "# IGK\n",
    "chord_diagram(matrix_igk, names=genes_igk, sort=\"size\", rotate_names=True,\n",
    "              fontsize=8, cmap=\"twilight\", ax=axes3[0])\n",
    "axes3[0].set_title(\"IGK Light Chain Vâ€“J Genes\", fontsize=12, y=1.1)\n",
    "\n",
    "# IGL\n",
    "chord_diagram(matrix_igl, names=genes_igl, sort=\"size\", rotate_names=True,\n",
    "              fontsize=8, cmap=\"twilight\", ax=axes3[1])\n",
    "axes3[1].set_title(\"IGL Light Chain Vâ€“J Genes\", fontsize=12, y=1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#fig3.savefig(\"chord_diagrams_light.png\", dpi=500, bbox_inches='tight', format='png')\n",
    "\n",
    "save_plot_to_memory(fig3, dpi=600)\n",
    "#<--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1747218078571,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "N2UcksaIQrwh",
    "outputId": "1dc1b4db-99d6-476c-eb5f-d7bee78ec5f7"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------->\n",
    "#  Export everything into Excel\n",
    "output_excel_path = f\"/content/drive/MyDrive/{output_csv_file}\"\n",
    "\n",
    "with pd.ExcelWriter(output_excel_path, engine='xlsxwriter') as writer:\n",
    "    # Save DataFrames\n",
    "    df_merged_b.to_excel(writer, sheet_name='Benchling_Data', index=False)\n",
    "    df_processed.to_excel(writer, sheet_name='Processed_Data', index=False)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    duplicate_resolution_df.to_excel(writer, sheet_name='Duplicate_Clones', index=False)\n",
    "\n",
    "    # Create a worksheet for plots/\n",
    "    workbook = writer.book\n",
    "    worksheet = workbook.add_worksheet(\"Plots\")\n",
    "    writer.sheets[\"Plots\"] = worksheet\n",
    "\n",
    "    # Insert each plot image\n",
    "    row = 0\n",
    "    for i, img in enumerate(plot_images):\n",
    "      worksheet.insert_image(row, 0, f\"plot_{i}.png\", {'image_data': img})\n",
    "      row += 40\n",
    "\n",
    "print(f\"Excel file successfully saved at: {output_excel_path}\")\n",
    "\n",
    "# OR Download the file LOCALLY\n",
    "#from google.colab import files\n",
    "#files.download(output_excel_path)\n",
    "#<--------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOIqBHo27tx2"
   },
   "source": [
    "#Update Notebook Version on *Github*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12244,
     "status": "ok",
     "timestamp": 1747217505629,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "ikGNPJWBa3FQ",
    "outputId": "4b268c9b-9fcc-462d-f448-39eeb8008dfd"
   },
   "outputs": [],
   "source": [
    "#UPDATE notebook version on github repo\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Input GitHub token securely\n",
    "token = getpass('Enter your GitHub token: ')\n",
    "\n",
    "# ðŸ§¾ Repo info\n",
    "username = \"Maria-Chatzinikolaou\"\n",
    "repo = \"Ab-Clone-Selenction\"\n",
    "repo_url = f\"https://{token}:x-oauth-basic@github.com/{username}/{repo}.git\"\n",
    "repo_path = f\"/content/{repo}\"\n",
    "\n",
    "# Configure Git\n",
    "!git config --global user.name \"Maria-Chatzinikolaou\"\n",
    "!git config --global user.email \"maria.chatzinikolaou@immune.engineering\"\n",
    "\n",
    "\n",
    "# Clone if not already cloned\n",
    "if not os.path.exists(repo):\n",
    "    !git clone {repo_url}\n",
    "\n",
    "# Set source and destination paths\n",
    "source_path = \"/content/drive/Shared drives/BIIE_PROGRAM-LABS/DEEPIR/3_RESEARCH/IRP-110_MAMMALIAN-AB/1_PILOT-PHASE/2_DATA-FILES/Sanger_Seq_Clones/SangerCloneIdentifier_DoublePrimer_CTLA4_high_binders.ipynb\"\n",
    "target_filename = \"SangerCloneIdentifier_DoublePrimer_CTLA4_high_binders.ipynb\"\n",
    "target_path = f\"{repo_path}/{target_filename}\"\n",
    "\n",
    "# Copy and rename notebook into the repo directory\n",
    "!cp \"{source_path}\" \"{target_path}\"\n",
    "\n",
    "# Clean outputs\n",
    "%cd {repo_path}\n",
    "!pip install nbstripout --quiet\n",
    "!nbstripout \"{target_filename}\"\n",
    "\n",
    "# Stage the notebook\n",
    "!git add \"{target_filename}\"\n",
    "\n",
    "# Commit\n",
    "!git commit -m \"test upload verion from Colab\"\n",
    "\n",
    "# Push to GitHub\n",
    "!git push {repo_url}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQooWAFCaHvL"
   },
   "source": [
    "# Microsynth NightSeq Plate Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1746625822821,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "NFSGVP2rakyQ",
    "outputId": "d7706d52-f3a6-4686-fcf0-6364fdb1516f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# ----- CONFIGURATION -----\n",
    "# Customize these components\n",
    "antigen = \"CTLA4\" #@param {type:\"string\")\n",
    "plate_id = \"P2\" #@param {type:\"string\")\n",
    "primer_name = \"PR171\" #@param {type:\"string\")\n",
    "primer_cat = \"Custom\" #@param {type:\"string\")\n",
    "output_file_path = f\"NightSeq_{plate_id}_{primer_name}_plate_order.xlsx\" #@param {type:\"string\")\n",
    "\n",
    "# 96-well layout (row-major: A1 to H12)\n",
    "rows = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "cols = list(range(1, 13))\n",
    "well_positions = [f\"{row}{col}\" for row in rows for col in cols]\n",
    "\n",
    "# ----- FUNCTION DEFINITIONS -----\n",
    "def reshape_row_major_to_column_order(sample_names):\n",
    "    n_rows, n_cols = 8, 12\n",
    "    matrix = [sample_names[i * n_cols:(i + 1) * n_cols] for i in range(n_rows)]\n",
    "    return [matrix[row][col] for col in range(n_cols) for row in range(n_rows)]\n",
    "\n",
    "def main():\n",
    "    sample_names = [f\"C{idx + 1}_{antigen}_{plate_id}_{well}\" for idx, well in enumerate(well_positions)]\n",
    "    primers = [primer_name] * len(sample_names)\n",
    "    primer_type = [primer_cat] * len(sample_names)\n",
    "\n",
    "    samples_reordered = reshape_row_major_to_column_order(sample_names)\n",
    "    primers_reordered = reshape_row_major_to_column_order(primers)\n",
    "    primer_type_recorded = reshape_row_major_to_column_order(primer_type)\n",
    "\n",
    "    df = pd.DataFrame(zip(samples_reordered, primers_reordered, primer_type_recorded))\n",
    "\n",
    "    with pd.ExcelWriter(output_file_path) as writer:\n",
    "        df.to_excel(writer, sheet_name=\"Plate\", index=False, header=False)\n",
    "\n",
    "    print(f\"Order Layout saved to {output_file_path}\")\n",
    "    return output_file_path\n",
    "\n",
    "# Run the script\n",
    "output_file = main()\n",
    "files.download(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eTXsAwDnUSY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZuz9qUVnLoz"
   },
   "source": [
    "# Check for clone overlap with the previous cov2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20540,
     "status": "ok",
     "timestamp": 1746685823212,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "YAz1WdAHlfSd",
    "outputId": "58281fa9-ccc9-4d75-e67b-eb2b7533f6d1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1747218202709,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "DV_YgWM_lfUq",
    "outputId": "07c92f6b-a3d3-4e90-830b-28aa18adab32"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load files from Google Drive\n",
    "xlsx_path_1 = \"/content/drive/MyDrive/CoV2_RBD_high_binders\"  # Update this\n",
    "xlsx_path_2 = \"/content/drive/MyDrive/CTLA4_high_binders\"      # Update this\n",
    "\n",
    "df_xlsx_1= pd.read_excel(xlsx_path_1)\n",
    "df_xlsx_2 = pd.read_excel(xlsx_path_2)\n",
    "\n",
    "# Step 2: Extract 'info' columns and drop any missing entries\n",
    "info_xlsx_1 = df_xlsx_1[\"info\"].dropna().astype(str).str.strip()\n",
    "info_xlsx_2 = df_xlsx_2[\"info\"].dropna().astype(str).str.strip()\n",
    "\n",
    "# Step 3: Find common gene combinations\n",
    "common_genes = set(info_xlsx_1).intersection(set(info_xlsx_2))\n",
    "\n",
    "# Step 4: Output result\n",
    "if common_genes:\n",
    "    print(\"âœ… Common gene combinations found:\")\n",
    "    for gene in common_genes:\n",
    "        print(gene)\n",
    "else:\n",
    "    print(\"âŒ No identical gene combinations found in 'info' columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "Q-vbbp1OlfWd",
    "outputId": "80d5b39f-70f0-4c24-e303-9768d200697d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "# --- Define a function to extract gene parts from info string ---\n",
    "def extract_genes(info_series):\n",
    "    gene_patterns = {\n",
    "        \"IGHV\": r\"IGHV[^\\s/]+\",\n",
    "        \"IGHD\": r\"IGHD[^\\s/]+\",\n",
    "        \"IGHJ\": r\"IGHJ[^\\s/]+\",\n",
    "        \"IGKV\": r\"IGKV[^\\s/]+\",\n",
    "        \"IGKJ\": r\"IGKJ[^\\s/]+\"\n",
    "    }\n",
    "    gene_counts = {k: [] for k in gene_patterns}\n",
    "\n",
    "    for entry in info_series:\n",
    "        for gene, pattern in gene_patterns.items():\n",
    "            matches = re.findall(pattern, entry)\n",
    "            gene_counts[gene].extend(matches)\n",
    "    return {k: Counter(v) for k, v in gene_counts.items()}\n",
    "\n",
    "# --- Extract gene counts from both files ---\n",
    "counts_xlsx_1 = extract_genes(info_xlsx_1)\n",
    "counts_xlsx_2 = extract_genes(info_xlsx_2)\n",
    "\n",
    "# --- Plotting ---\n",
    "for gene_type in counts_xlsx_1:\n",
    "    combined_keys = set(counts_xlsx_1[gene_type]) | set(counts_xlsx_2[gene_type])\n",
    "    data = {\n",
    "        \"Gene\": list(combined_keys),\n",
    "        \"cov2_high_binders\": [counts_xlsx_1[gene_type].get(g, 0) for g in combined_keys],\n",
    "        \"ctla4_high_binders\": [counts_xlsx_2[gene_type].get(g, 0) for g in combined_keys]\n",
    "    }\n",
    "    df_plot = pd.DataFrame(data).sort_values(\"ctla4_high_binders\", ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bar_width = 0.4\n",
    "    x = range(len(df_plot))\n",
    "\n",
    "    plt.bar([i - bar_width/2 for i in x], df_plot[\"cov2_high_binders\"], width=bar_width, label=\"cov2_high_binders\", color='blue', alpha=0.7)\n",
    "    plt.bar([i + bar_width/2 for i in x], df_plot[\"ctla4_high_binders\"], width=bar_width, label=\"ctla4_high_binders\", color='orange', alpha=0.7)\n",
    "\n",
    "    plt.xticks(ticks=x, labels=df_plot[\"Gene\"], rotation=45, ha='right')\n",
    "    plt.title(f\"Frequency of {gene_type} Genes\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyV9COh_my29"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRW_2yIMuXDS"
   },
   "source": [
    "# Create an input file for QPix to select out the clones to be rearranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88D9gJXkw6ON"
   },
   "outputs": [],
   "source": [
    "plate = \"PLATE: 1\" #@param {type:\"string\"}\n",
    "barcode = \"BARCODE: 2\" #@param {type:\"string\"}\n",
    "type = \"TYPE: SOURCE\" #@param {type:\"string\"}\n",
    "plate_type = \"PLATETYPE: Nunc 96 Deep Well\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746705631609,
     "user": {
      "displayName": "Maria Chatzinikolaou",
      "userId": "16453118161306485969"
     },
     "user_tz": -120
    },
    "id": "wDZJuO05u0OB",
    "outputId": "6a043868-093f-4659-91d9-068beb64d61e"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Extract well names from the last part of \"clone_name\"\n",
    "df_merged_b[\"well\"] = df_merged_b[\"clone_name\"].str.extract(r'_([^_]+)$')\n",
    "\n",
    "# Extract unique well names\n",
    "well_names = df_merged_b[\"well\"].dropna().unique().tolist()\n",
    "\n",
    "# Prepare content for the file\n",
    "output_lines = [\n",
    "    \"PLATE: 1\",\n",
    "    \"BARCODE: 2\",\n",
    "    \"TYPE: SOURCE\",\n",
    "    \"PLATETYPE: Nunc 96 Deep Well\",\n",
    "    \"SUBTYPE:\"\n",
    "] + well_names\n",
    "\n",
    "\n",
    "output_file = '/content/QPix_plate_layout.frd'\n",
    "\n",
    "# Write with CR-LF line endings **and** a trailing newline\n",
    "content = \"\\r\\n\".join(output_lines) + \"\\r\\n\"        # <-- extra CRLF at the end\n",
    "with open(output_file, \"wb\") as f:                  # binary mode\n",
    "    f.write(content.encode(\"utf-8\"))                # explicit UTF-8, no BOM\n",
    "\n",
    "# Trigger download\n",
    "files.download(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4cLD-ipu1gg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ7W5mmfu1im"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_OAITpMu1k9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
